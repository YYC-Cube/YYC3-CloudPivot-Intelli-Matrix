/**
 * useBigModelSDK.ts
 * ==================
 * BigModel SDK æ¡¥æ¥å±‚ Hook
 *
 * åŠŸèƒ½:
 * - å°† ConfiguredModel â†’ å®é™… API è°ƒç”¨
 * - ç»Ÿä¸€ chat / chatStream æ¥å£ (æ”¯æŒ Z.ai / OpenAI / Ollama / å…¶ä»– OpenAI-compatible)
 * - Mock æ¨¡å¼: æ—  API Key æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
 * - ä½¿ç”¨ç»Ÿè®¡ (localStorage æŒä¹…åŒ–)
 * - ä¼šè¯ç®¡ç† (localStorage æŒä¹…åŒ–)
 */

import { useState, useCallback, useMemo, useRef } from "react";
import type {
  ConfiguredModel,
  ModelProviderId,
  ChatMessage,
  ChatSession,
  ChatRole,
  SDKConnectionStatus,
  SDKUsageStats,
  SDKChatResponse,
  SDKCapability,
  SDKProviderCapabilities,
} from "../types";

// ============================================================
// å¸¸é‡
// ============================================================

const SESSIONS_KEY = "yyc3_chat_sessions";
const STATS_KEY = "yyc3_sdk_usage_stats";

/** å„æä¾›å•†æ”¯æŒçš„èƒ½åŠ› */
export const PROVIDER_CAPABILITIES: SDKProviderCapabilities[] = [
  { providerId: "zhipu",          capabilities: ["chat", "chat-stream", "file-upload", "knowledge-base", "image-gen", "tts", "stt", "video-gen", "code-gen"] },
  { providerId: "zhipu-plan",     capabilities: ["chat", "chat-stream"] },
  { providerId: "openai",         capabilities: ["chat", "chat-stream", "image-gen", "tts", "stt", "code-gen"] },
  { providerId: "kimi-cn",        capabilities: ["chat", "chat-stream"] },
  { providerId: "kimi-global",    capabilities: ["chat", "chat-stream"] },
  { providerId: "deepseek",       capabilities: ["chat", "chat-stream", "code-gen"] },
  { providerId: "volcengine",     capabilities: ["chat", "chat-stream"] },
  { providerId: "volcengine-plan", capabilities: ["chat", "chat-stream"] },
  { providerId: "ollama",         capabilities: ["chat", "chat-stream", "code-gen"] },
];

// ============================================================
// å·¥å…·å‡½æ•°
// ============================================================

function loadSessions(): ChatSession[] {
  try {
    const raw = localStorage.getItem(SESSIONS_KEY);
    return raw ? JSON.parse(raw) : [];
  } catch { return []; }
}

function saveSessions(sessions: ChatSession[]) {
  try { localStorage.setItem(SESSIONS_KEY, JSON.stringify(sessions)); } catch {
    // Storage unavailable
  }
}

function loadStats(): SDKUsageStats {
  try {
    const raw = localStorage.getItem(STATS_KEY);
    return raw ? JSON.parse(raw) : defaultStats();
  } catch { return defaultStats(); }
}

function saveStats(stats: SDKUsageStats) {
  try { localStorage.setItem(STATS_KEY, JSON.stringify(stats)); } catch {
    // Storage unavailable
  }
}

function defaultStats(): SDKUsageStats {
  return {
    totalRequests: 0,
    totalTokensIn: 0,
    totalTokensOut: 0,
    avgLatencyMs: 0,
    lastRequestAt: null,
    errorCount: 0,
  };
}

function genId(): string {
  return `${Date.now()}-${Math.random().toString(36).slice(2, 8)}`;
}

/** æ„å»º Authorization header */
function buildHeaders(model: ConfiguredModel): Record<string, string> {
  const headers: Record<string, string> = { "Content-Type": "application/json" };

  if (model.providerId === "zhipu" || model.providerId === "zhipu-plan") {
    // Z.ai ä½¿ç”¨ Bearer token (å®˜æ–¹ SDK ç”¨ Bearer)
    headers["Authorization"] = `Bearer ${model.apiKey}`;
  } else if (model.providerId === "ollama") {
    // Ollama æ— éœ€ auth
  } else {
    // OpenAI / Kimi / DeepSeek / ç«å±±å¼•æ“ å‡ä¸º Bearer
    headers["Authorization"] = `Bearer ${model.apiKey}`;
  }

  return headers;
}

/** æ„å»º chat endpoint URL */
function buildChatUrl(model: ConfiguredModel): string {
  const base = model.baseUrl.replace(/\/$/, "");

  if (model.providerId === "ollama") {
    return `${base}/api/chat`;
  }

  // Z.ai / OpenAI / Kimi / DeepSeek / ç«å±±å¼•æ“ éƒ½ç”¨ OpenAI-compatible ç«¯ç‚¹
  return `${base}/chat/completions`;
}

/** æ„å»ºè¯·æ±‚ body */
function buildRequestBody(
  model: ConfiguredModel,
  messages: { role: ChatRole; content: string }[],
  stream: boolean,
  temperature: number,
): Record<string, unknown> {
  if (model.providerId === "ollama") {
    return {
      model: model.model,
      messages,
      stream,
    };
  }

  return {
    model: model.model,
    messages,
    temperature,
    stream,
  };
}

// ============================================================
// Mock æµå¼å“åº”æ¨¡æ‹Ÿ
// ============================================================

const MOCK_RESPONSES: Record<string, string> = {
  default: "ä½ å¥½ï¼æˆ‘æ˜¯ YYC3 CloudPivot Intelli-Matrix çš„ AI åŠ©æ‰‹ã€‚å½“å‰å¤„äº Mock æ¨¡å¼ï¼Œå¦‚éœ€çœŸå® API è°ƒç”¨è¯·å…ˆåœ¨ã€Œæ¨¡å‹ç®¡ç†ã€ä¸­é…ç½®æœ‰æ•ˆçš„ API Keyã€‚\n\næˆ‘å¯ä»¥ååŠ©ä½ è¿›è¡Œï¼š\n- ç³»ç»ŸçŠ¶æ€åˆ†æ\n- å¼‚å¸¸æ¨¡å¼è¯†åˆ«\n- æ“ä½œå»ºè®®æ¨è\n- Text-to-SQL æŸ¥è¯¢\n\nè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ",
  status: "**ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ (Mock)**\n\n| æŒ‡æ ‡ | å€¼ | çŠ¶æ€ |\n|------|-----|------|\n| æ´»è·ƒèŠ‚ç‚¹ | 12/13 | âœ… æ­£å¸¸ |\n| GPU åˆ©ç”¨ç‡ | 78.5% | âš ï¸ åé«˜ |\n| Token åå | 2,847/s | âœ… æ­£å¸¸ |\n| å­˜å‚¨å ç”¨ | 85.8% | âš ï¸ æ¥è¿‘é˜ˆå€¼ |\n\nå»ºè®®å…³æ³¨ NAS-Storage-01 å­˜å‚¨å®¹é‡ã€‚",
  error: "**æ£€æµ‹åˆ°ä»¥ä¸‹å¼‚å¸¸æ¨¡å¼ (Mock)**\n\n1. ğŸ”´ GPU-A100-03 æ¨ç†å»¶è¿Ÿ 2,450ms > 2,000ms é˜ˆå€¼\n2. âš ï¸ æ˜¾å­˜ä½¿ç”¨ç‡ 89% æ¥è¿‘ OOM\n3. âš ï¸ NAS å­˜å‚¨ 85.8% é¢„è®¡ 7 å¤©è¾¾è­¦æˆ’çº¿\n\n**æ¨èæ“ä½œ**: å°† LLaMA-70B è¿ç§»åˆ° GPU-A100-07ï¼ˆå½“å‰è´Ÿè½½ 15%ï¼‰",
};

function getMockResponse(input: string): string {
  const lower = input.toLowerCase();
  if (lower.includes("çŠ¶æ€") || lower.includes("status")) {return MOCK_RESPONSES.status;}
  if (lower.includes("å¼‚å¸¸") || lower.includes("error") || lower.includes("å‘Šè­¦")) {return MOCK_RESPONSES.error;}
  return MOCK_RESPONSES.default;
}

// ============================================================
// Hook
// ============================================================

export function useBigModelSDK() {
  const [sessions, setSessions] = useState<ChatSession[]>(loadSessions);
  const [activeSessionId, setActiveSessionId] = useState<string | null>(null);
  const [connectionStatus, setConnectionStatus] = useState<SDKConnectionStatus>("idle");
  const [usageStats, setUsageStats] = useState<SDKUsageStats>(loadStats);
  const [streaming, setStreaming] = useState(false);
  const [streamingContent, setStreamingContent] = useState("");
  const [error, setError] = useState<string | null>(null);
  const abortRef = useRef<AbortController | null>(null);

  // ========== ä¼šè¯ç®¡ç† ==========

  const activeSession = useMemo(
    () => sessions.find((s) => s.id === activeSessionId) ?? null,
    [sessions, activeSessionId],
  );

  const createSession = useCallback((modelId: string, title?: string): ChatSession => {
    const session: ChatSession = {
      id: genId(),
      title: title || `Chat ${new Date().toLocaleTimeString()}`,
      modelId,
      messages: [],
      createdAt: Date.now(),
      updatedAt: Date.now(),
    };
    setSessions((prev) => {
      const next = [session, ...prev];
      saveSessions(next);
      return next;
    });
    setActiveSessionId(session.id);
    return session;
  }, []);

  const deleteSession = useCallback((id: string) => {
    setSessions((prev) => {
      const next = prev.filter((s) => s.id !== id);
      saveSessions(next);
      return next;
    });
    setActiveSessionId((prev) => (prev === id ? null : prev));
  }, []);

  const clearSessions = useCallback(() => {
    setSessions([]);
    saveSessions([]);
    setActiveSessionId(null);
  }, []);

  // ========== èƒ½åŠ›æŸ¥è¯¢ ==========

  const getCapabilities = useCallback((providerId: ModelProviderId): SDKCapability[] => {
    return PROVIDER_CAPABILITIES.find((p) => p.providerId === providerId)?.capabilities ?? [];
  }, []);

  const hasCapability = useCallback((providerId: ModelProviderId, cap: SDKCapability): boolean => {
    return getCapabilities(providerId).includes(cap);
  }, [getCapabilities]);

  // ========== æ ¸å¿ƒ: å‘é€æ¶ˆæ¯ (åŒæ­¥) ==========

  const sendMessage = useCallback(async (
    configuredModel: ConfiguredModel,
    userContent: string,
    sessionId?: string,
  ): Promise<SDKChatResponse> => {
    setError(null);
    setConnectionStatus("connecting");

    const isMock = !configuredModel.apiKey && configuredModel.providerId !== "ollama";
    const targetSessionId = sessionId || activeSessionId;

    // åˆ›å»º user message
    const userMsg: ChatMessage = {
      id: genId(),
      role: "user",
      content: userContent,
      timestamp: Date.now(),
      model: configuredModel.model,
      provider: configuredModel.providerId,
    };

    // è¿½åŠ åˆ° session
    const updateMessages = (newMsg: ChatMessage) => {
      setSessions((prev) => {
        const next = prev.map((s) =>
          s.id === targetSessionId
            ? { ...s, messages: [...s.messages, newMsg], updatedAt: Date.now() }
            : s,
        );
        saveSessions(next);
        return next;
      });
    };

    updateMessages(userMsg);

    const start = Date.now();

    if (isMock) {
      // Mock æ¨¡å¼
      await new Promise((r) => setTimeout(r, 300 + Math.random() * 700));
      const mockContent = getMockResponse(userContent);
      const mockTokens = { input: userContent.length, output: mockContent.length };

      const assistantMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content: mockContent,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
        tokens: mockTokens,
      };
      updateMessages(assistantMsg);

      const latency = Date.now() - start;
      const response: SDKChatResponse = {
        id: genId(),
        model: configuredModel.model,
        content: mockContent,
        finishReason: "stop",
        usage: { promptTokens: mockTokens.input, completionTokens: mockTokens.output, totalTokens: mockTokens.input + mockTokens.output },
        latencyMs: latency,
      };

      updateStats(response);
      setConnectionStatus("connected");
      return response;
    }

    // çœŸå® API è°ƒç”¨
    try {
      const currentSession = sessions.find((s) => s.id === targetSessionId);
      const history = currentSession?.messages.map((m) => ({ role: m.role, content: m.content })) ?? [];
      history.push({ role: "user" as ChatRole, content: userContent });

      const url = buildChatUrl(configuredModel);
      const headers = buildHeaders(configuredModel);
      const body = buildRequestBody(configuredModel, history, false, 0.7);

      abortRef.current = new AbortController();
      const res = await fetch(url, {
        method: "POST",
        headers,
        body: JSON.stringify(body),
        signal: abortRef.current.signal,
      });

      if (!res.ok) {
        throw new Error(`API ${res.status}: ${res.statusText}`);
      }

      const data = await res.json();
      const latency = Date.now() - start;

      // è§£æå“åº” (OpenAI-compatible æ ¼å¼)
      let content: string;
      let usage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };

      if (configuredModel.providerId === "ollama") {
        content = data.message?.content ?? "";
        usage = {
          promptTokens: data.prompt_eval_count ?? 0,
          completionTokens: data.eval_count ?? 0,
          totalTokens: (data.prompt_eval_count ?? 0) + (data.eval_count ?? 0),
        };
      } else {
        content = data.choices?.[0]?.message?.content ?? "";
        usage = {
          promptTokens: data.usage?.prompt_tokens ?? 0,
          completionTokens: data.usage?.completion_tokens ?? 0,
          totalTokens: data.usage?.total_tokens ?? 0,
        };
      }

      const assistantMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
        tokens: { input: usage.promptTokens, output: usage.completionTokens },
      };
      updateMessages(assistantMsg);

      const response: SDKChatResponse = {
        id: data.id ?? genId(),
        model: configuredModel.model,
        content,
        finishReason: configuredModel.providerId === "ollama" ? "stop" : (data.choices?.[0]?.finish_reason ?? "stop"),
        usage,
        latencyMs: latency,
      };

      updateStats(response);
      setConnectionStatus("connected");
      return response;
    } catch (err: any) {
      const latency = Date.now() - start;
      setConnectionStatus("error");
      setError(err.message);

      setUsageStats((prev) => {
        const next = { ...prev, errorCount: prev.errorCount + 1 };
        saveStats(next);
        return next;
      });

      // è¿”å›é”™è¯¯å“åº” & è¿½åŠ é”™è¯¯æ¶ˆæ¯
      const errorMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content: `[ERROR] ${err.message}`,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
      };
      updateMessages(errorMsg);

      return {
        id: genId(),
        model: configuredModel.model,
        content: `[ERROR] ${err.message}`,
        finishReason: "error",
        usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        latencyMs: latency,
      };
    }
  }, [activeSessionId, sessions]);

  // ========== æ ¸å¿ƒ: æµå¼å‘é€ ==========

  const sendMessageStream = useCallback(async (
    configuredModel: ConfiguredModel,
    userContent: string,
    sessionId?: string,
    onChunk?: (chunk: string) => void,
  ): Promise<SDKChatResponse> => {
    setError(null);
    setStreaming(true);
    setStreamingContent("");
    setConnectionStatus("connecting");

    const isMock = !configuredModel.apiKey && configuredModel.providerId !== "ollama";
    const targetSessionId = sessionId || activeSessionId;

    const userMsg: ChatMessage = {
      id: genId(),
      role: "user",
      content: userContent,
      timestamp: Date.now(),
      model: configuredModel.model,
      provider: configuredModel.providerId,
    };

    const updateMessages = (newMsg: ChatMessage) => {
      setSessions((prev) => {
        const next = prev.map((s) =>
          s.id === targetSessionId
            ? { ...s, messages: [...s.messages, newMsg], updatedAt: Date.now() }
            : s,
        );
        saveSessions(next);
        return next;
      });
    };

    updateMessages(userMsg);

    const start = Date.now();

    if (isMock) {
      // Mock æµå¼
      const mockContent = getMockResponse(userContent);
      const chars = mockContent.split("");
      let accumulated = "";

      for (const char of chars) {
        await new Promise((r) => setTimeout(r, 15 + Math.random() * 25));
        accumulated += char;
        setStreamingContent(accumulated);
        onChunk?.(char);
      }

      const mockTokens = { input: userContent.length, output: mockContent.length };
      const assistantMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content: mockContent,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
        tokens: mockTokens,
      };
      updateMessages(assistantMsg);

      const latency = Date.now() - start;
      const response: SDKChatResponse = {
        id: genId(),
        model: configuredModel.model,
        content: mockContent,
        finishReason: "stop",
        usage: { promptTokens: mockTokens.input, completionTokens: mockTokens.output, totalTokens: mockTokens.input + mockTokens.output },
        latencyMs: latency,
      };

      updateStats(response);
      setStreaming(false);
      setStreamingContent("");
      setConnectionStatus("connected");
      return response;
    }

    // çœŸå®æµå¼ API
    try {
      const currentSession = sessions.find((s) => s.id === targetSessionId);
      const history = currentSession?.messages.map((m) => ({ role: m.role, content: m.content })) ?? [];
      history.push({ role: "user" as ChatRole, content: userContent });

      const url = buildChatUrl(configuredModel);
      const headers = buildHeaders(configuredModel);
      const body = buildRequestBody(configuredModel, history, true, 0.7);

      abortRef.current = new AbortController();
      const res = await fetch(url, {
        method: "POST",
        headers,
        body: JSON.stringify(body),
        signal: abortRef.current.signal,
      });

      if (!res.ok) {throw new Error(`API ${res.status}: ${res.statusText}`);}

      setConnectionStatus("connected");

      const reader = res.body?.getReader();
      if (!reader) {throw new Error("Response body is not readable");}

      const decoder = new TextDecoder();
      let buffer = "";
      let fullContent = "";

      for (;;) {
        const { done, value } = await reader.read();
        if (done) {break;}

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";

        for (const line of lines) {
          if (line.startsWith("data: ")) {
            const data = line.slice(6).trim();
            if (data === "[DONE]") {continue;}
            try {
              const chunk = JSON.parse(data);
              const delta = configuredModel.providerId === "ollama"
                ? chunk.message?.content ?? ""
                : chunk.choices?.[0]?.delta?.content ?? "";
              if (delta) {
                fullContent += delta;
                setStreamingContent(fullContent);
                onChunk?.(delta);
              }
            } catch { /* skip malformed chunks */ }
          }
        }
      }

      const assistantMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content: fullContent,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
        tokens: { input: userContent.length, output: fullContent.length },
      };
      updateMessages(assistantMsg);

      const latency = Date.now() - start;
      const response: SDKChatResponse = {
        id: genId(),
        model: configuredModel.model,
        content: fullContent,
        finishReason: "stop",
        usage: { promptTokens: userContent.length, completionTokens: fullContent.length, totalTokens: userContent.length + fullContent.length },
        latencyMs: latency,
      };

      updateStats(response);
      setStreaming(false);
      setStreamingContent("");
      return response;
    } catch (err: any) {
      setConnectionStatus("error");
      setError(err.message);
      setStreaming(false);
      setStreamingContent("");

      setUsageStats((prev) => {
        const next = { ...prev, errorCount: prev.errorCount + 1 };
        saveStats(next);
        return next;
      });

      const errorMsg: ChatMessage = {
        id: genId(),
        role: "assistant",
        content: `[ERROR] ${err.message}`,
        timestamp: Date.now(),
        model: configuredModel.model,
        provider: configuredModel.providerId,
      };
      updateMessages(errorMsg);

      return {
        id: genId(),
        model: configuredModel.model,
        content: `[ERROR] ${err.message}`,
        finishReason: "error",
        usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        latencyMs: Date.now() - start,
      };
    }
  }, [activeSessionId, sessions]);

  // ========== ä¸­æ–­è¯·æ±‚ ==========

  const abort = useCallback(() => {
    abortRef.current?.abort();
    setStreaming(false);
    setStreamingContent("");
  }, []);

  // ========== ç»Ÿè®¡æ›´æ–° ==========

  const updateStats = useCallback((response: SDKChatResponse) => {
    setUsageStats((prev) => {
      const totalReqs = prev.totalRequests + 1;
      const newAvg = ((prev.avgLatencyMs * prev.totalRequests) + response.latencyMs) / totalReqs;
      const next: SDKUsageStats = {
        totalRequests: totalReqs,
        totalTokensIn: prev.totalTokensIn + response.usage.promptTokens,
        totalTokensOut: prev.totalTokensOut + response.usage.completionTokens,
        avgLatencyMs: Math.round(newAvg),
        lastRequestAt: Date.now(),
        errorCount: prev.errorCount,
      };
      saveStats(next);
      return next;
    });
  }, []);

  // ========== è¿æ¥æµ‹è¯• (çœŸå® API è°ƒç”¨) ==========

  const testConnection = useCallback(async (configuredModel: ConfiguredModel): Promise<{
    success: boolean;
    latencyMs: number;
    error?: string;
  }> => {
    const start = Date.now();

    if (!configuredModel.apiKey && configuredModel.providerId !== "ollama") {
      // Mock test
      await new Promise((r) => setTimeout(r, 200 + Math.random() * 300));
      return { success: true, latencyMs: Date.now() - start };
    }

    try {
      if (configuredModel.providerId === "ollama") {
        const res = await fetch(`${configuredModel.baseUrl.replace(/\/$/, "")}/api/tags`, {
          signal: AbortSignal.timeout(5000),
        });
        if (!res.ok) {throw new Error(`HTTP ${res.status}`);}
        return { success: true, latencyMs: Date.now() - start };
      }

      // OpenAI-compatible: å‘ä¸€ä¸ªæœ€å°è¯·æ±‚
      const url = buildChatUrl(configuredModel);
      const headers = buildHeaders(configuredModel);
      const body = buildRequestBody(
        configuredModel,
        [{ role: "user" as ChatRole, content: "ping" }],
        false,
        0,
      );

      const res = await fetch(url, {
        method: "POST",
        headers,
        body: JSON.stringify({ ...body, max_tokens: 1 }),
        signal: AbortSignal.timeout(10000),
      });

      if (!res.ok) {
        const text = await res.text();
        throw new Error(`HTTP ${res.status}: ${text.slice(0, 200)}`);
      }

      return { success: true, latencyMs: Date.now() - start };
    } catch (err: any) {
      return { success: false, latencyMs: Date.now() - start, error: err.message };
    }
  }, []);

  // ========== é‡ç½®ç»Ÿè®¡ ==========

  const resetStats = useCallback(() => {
    const empty = defaultStats();
    setUsageStats(empty);
    saveStats(empty);
  }, []);

  return {
    // ä¼šè¯ç®¡ç†
    sessions,
    activeSession,
    activeSessionId,
    setActiveSessionId,
    createSession,
    deleteSession,
    clearSessions,

    // çŠ¶æ€
    connectionStatus,
    streaming,
    streamingContent,
    error,

    // æ ¸å¿ƒæ–¹æ³•
    sendMessage,
    sendMessageStream,
    abort,
    testConnection,

    // èƒ½åŠ›
    getCapabilities,
    hasCapability,

    // ç»Ÿè®¡
    usageStats,
    resetStats,
  };
}